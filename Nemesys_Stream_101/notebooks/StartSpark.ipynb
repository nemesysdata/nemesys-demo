{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f67035-2c51-46a4-a84d-bfc28435975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run load_environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60cf9d-5f50-4214-ab85-b67e7a58c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a437797-a51b-4a4a-b83d-870961e024dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"NemesysNotebook\"\n",
    "try:\n",
    "    appName = APP_NAME\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207525-52ae-4892-b66c-6c3214bdf4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(appName)\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate()\n",
    ")\n",
    "         # .enableHiveSupport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed7586-8f63-4822-8e0b-392b688c2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "# Configurar o acesso ao Azure Data Lake (Blob)\n",
    "#--------------------------------------------------------------------------\n",
    "spark.conf.set(f'fs.azure.account.auth.type.{STORAGE_ACCOUNT}.dfs.core.windows.net', 'SharedKey')\n",
    "spark.conf.set(f'fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net', STORAGE_KEY)\n",
    "#--------------------------------------------------------------------------\n",
    "# Configurar o acesso ao S3\n",
    "#--------------------------------------------------------------------------\n",
    "spark.conf.set('fs.s3a.endpoint', S3_URL)\n",
    "spark.conf.set('fs.s3a.access.key', S3_ACCESS_KEY)\n",
    "spark.conf.set('fs.s3a.secret.key', S3_SECRET_KEY)\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "spark.conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.conf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a54790-7eeb-4724-94e2-e9b283ce860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "# Checar se Delta Table existe\n",
    "#--------------------------------------------------------------------------\n",
    "def delta_exists(delta_path, tier, table):\n",
    "    if delta_path.starts_with(\"abfss://\"):\n",
    "        url = f'DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_key};EndpointSuffix=core.windows.net'\n",
    "        blob = BlobClient.from_connection_string(conn_str=url, container_name=BLOB_CONTAINER, blob_name=f'{LAKEHOUSE_PATH}/bronze/{db}_{topic}')\n",
    "        return blob.exists()\n",
    "    elif delta_path.starts_with(\"s3a\"):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return path.exists(delta_path)\n",
    "#--------------------------------------------------------------------------\n",
    "# Retornar os caminhos da tabela\n",
    "#--------------------------------------------------------------------------    \n",
    "def paths(topico: str, db: str, tier: str):\n",
    "    delta_path = f'abfss://{BLOB_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{LAKEHOUSE_PATH}/{tier}/{db}_{topico}'\n",
    "    checkpoint_path = f'abfss://{BLOB_CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/{LAKEHOUSE_PATH}/{tier}/checkpoint/kafka/{db}_{topico}'\n",
    "\n",
    "    return delta_path, checkpoint_path\n",
    "#--------------------------------------------------------------------------\n",
    "# Carregar uma delta table e registrar como tempor√°ria\n",
    "#--------------------------------------------------------------------------\n",
    "def loadAndRegister(table:str, db:str, tier:str = \"bronze\"):\n",
    "    delta_path, _ = paths(table, db, tier)\n",
    "    df = spark.read.format(\"delta\").load(delta_path)\n",
    "    df.createOrReplaceTempView(f\"{tier}_{table}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f44e7-3ad5-49e3-a257-0618cb16e1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
