{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4474b0d-9e5b-480e-96dc-41bc53480b21",
   "metadata": {},
   "source": [
    "# Gerar Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45722d8-f82a-4c39-ac29-ee6dc3c70e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.pandas as ps\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import dates\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.avro.functions import *\n",
    "#\n",
    "# Nome da aplicação Spark\n",
    "#\n",
    "APP_NAME=\"GerarBronze\"\n",
    "ps.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1179e3-e220-4206-a7cd-72bbe227da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run StartSpark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1912445-960a-48d8-bd8b-2161e83b386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.lazy_execution = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccbed15-f1cf-498d-b45c-a8f2ee5b0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3205f96-4dfd-4eb4-934b-b6e0391ee00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Stock\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"ticker\", \"type\": \"string\"},\n",
    "            {\"name\": \"timestamp\", \"type\": \"string\"},\n",
    "            {\"name\": \"open\", \"type\": \"double\"},\n",
    "            {\"name\": \"high\", \"type\": \"double\"},\n",
    "            {\"name\": \"low\", \"type\": \"double\"},\n",
    "            {\"name\": \"close\", \"type\": \"double\"},\n",
    "            {\"name\": \"volume\", \"type\": \"long\"}\n",
    "        ]\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6dd65b-f67e-4c32-b8e1-584e5359385e",
   "metadata": {},
   "source": [
    "# FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3484c8-1e3b-43f9-9340-e7d88d8b6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def delta_exists(delta_path, tier, table):\n",
    "    url = urlparse(delta_path)\n",
    "\n",
    "    match url:\n",
    "        case \"abfss\":\n",
    "            print(\"Blob Storage\")\n",
    "        case \"s3\":\n",
    "        case \"s3a\":\n",
    "            print(\"S3 Compatible\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adedb382-ef23-4fc9-92d9-96c3bf24ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def delta_exists(delta_path, tier, table):\n",
    "  \"\"\"\n",
    "  Checks if the provided delta path points to a supported storage type.\n",
    "\n",
    "  Args:\n",
    "      delta_path (str): The URL or path to the delta data.\n",
    "      tier (str): Optional tier information (may not be used).\n",
    "      table (str): Optional table name (may not be used).\n",
    "\n",
    "  Returns:\n",
    "      str: A string indicating the storage type (\"Blob Storage\" or \"S3 Compatible\")\n",
    "          or None if the storage type is not supported.\n",
    "  \"\"\"\n",
    "\n",
    "  url = urlparse(delta_path)\n",
    "  match url.scheme:\n",
    "      case \"abfss\":\n",
    "          return \"Blob Storage\"\n",
    "      case \"s3\" | \"s3a\":  # Combine S3 and S3A cases for efficiency\n",
    "          return \"S3 Compatible\"\n",
    "      case _:\n",
    "          return None  # Return None for unsupported schemes\n",
    "\n",
    "  # Unreachable code, but included for clarity\n",
    "  # return None  # Redundant return statement here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6ca4ffa-710d-4883-89a6-eecba0e4de22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S3 Compatible'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_exists(\"s3a://nemesys-demo1/lakehouse\", \"bronze\", \"stocks_intraday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1328cdf8-d13d-4cbc-a4e6-8f6c695c791e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blob Storage'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_exists(\"abfss://nemesys-demo1/lakehouse\", \"bronze\", \"stocks_intraday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784256cd-8491-4a4b-99da-ff3847dfa7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
